#!/usr/bin/env python3

import io
import sys
import atexit
import os
import re
import netrc
import configparser
import argparse
from pybis import Openbis
from dateutil import parser


# parse command line
argparser = argparse.ArgumentParser(description="Fetch metadata from OpenBIS server using PyBIS APIs")
argparser.add_argument('-c', '--config', metavar='CONF', required=False,
	default='server.conf',
	type=str, dest='config', help="configuration file to load")
argparser.add_argument('-s', '--summary', required=False,
	action='store_true', dest='summary', help="Only display a summary of datasets, not an exhaustive list of all samples")
args = argparser.parse_args()


# Load defaults from config file
config = configparser.ConfigParser(strict=False) # non-strict: support repeated section headers
config.SECTCRE = re.compile(r'\[ *(?P<header>[^]]+?) *\]') # support spaces in section headers
with open(args.config) as f: config.read_string(f"""
[DEFAULT]
samtype=ILLUMINA_FLOW_LANE
basedir={os.getcwd()}
sampleset=sampleset
download=openbis-downloads
suffix=
link=--link
[_]
""" + f.read()) # add defaults + a pseudo-section "_" right before the ini file, to support bash-style section_header-less config files

fileserver=config['_']['fileserver'].strip("\"'")
'''name of the _SFTP_ server, as used when fetching data files (i.e.: same name as in netrc file)'''
apiurl=config['_']['apiurl'].strip("\"'")
'''address of the _Web_ server whose API we are calling'''
expname=config['_']['expname'].strip("\"'")
'''experiment name in OpenBIS'''
samtype=config['_']['samtype'].strip("\"'")
'''the type for which need to search the experiment thourgh'''
basedir=config['_']['basedir'].strip("\"'")
'''base dircetory'''
download=config['_']['download'].strip("\"'")
'''sub-directory to hold the unsorted downloaded datasets'''
sampleset=config['_']['sampleset'].strip("\"'")
'''sub-directory to hold the sorted samples set'''
suffix=config['_']['suffix'].strip("\"'")
'''extra suffix - e.g.: "_MM_1" for mismatches one'''
link=config['_']['link'].strip("\"'")
'''
linking instead of copying ?
 --reflink for CoW filesystems (ZFS, BTRFS)
 --hardlink for most unix-like filesystems
'''

# RegEx to parse some specific string
rxrun=re.compile('(?ms)Run type: (?P<type>\S+).*Number of cycles: (?P<len>\d+).*Run folder: (?P<run>(?P<date>\d{6})_[-\w]*)') # e.g.: 'Run type: PAIRED_END\nNumber of cycles: 251\n\nKit : MS9275058-600V3\nRun folder: 200430_M01761_0414_000000000-J3JCT' or {...}'folder: 201023_A00730_0259_BHTVCCDRXX'
rxcell=re.compile('(?<=/)(?P<seq>(?:\w+-)?(?P<cell>\w+)(?:\:(?P<lane>\d+))?)$') # e.g.: '/BSSE_STADLER_COVID/000000000-CTT3D:1' or '/BSSE_STADLER_COVID/HTVCCDRXX:2'
rxclean=re.compile('[\W_]+') # or [\-\.\:\/] : characters that are converted to '_' for file-system friendliness

# create sampleset directory if missing
if not os.path.isdir(os.path.join(basedir,sampleset)):
	try:
		os.mkdir(os.path.join(basedir,sampleset), mode=0o770)
	except FileExistsError:
		pass

# we use ~/.netrc to obtain credentials 
# (we need that config file anyway to download the data files from openbis' fileserver)
username,password=netrc.netrc().authenticators(fileserver)[0::2];

## console interactive
#import getpass
#username='ivan.topolsky@bsse.ethz.ch'
#password=getpass.getpass()

o=Openbis(apiurl, verify_certificates=True)
o.login(username, password, save_token=True)   # save the session token in ~/.pybis/example.com.token
atexit.register(o.logout)

# shell script file with all moving instructions inside
sh=open(os.path.join(basedir,sampleset,'movedatafiles.sh'), 'wt')
print(r'''
link='%(link)s'

# Helper
fail() {
	printf '\e[31;1mArgh: %%s\e[0m\n'	"$1"	1>&2
	[[ -n "$2" ]] && echo "$2" 1>&2
	exit 1
}

ALLOK=1
X() {
	ALLOK=0
}

cd %(basedir)s

# sanity checks
[[ -d '%(download)s' ]] || fail 'No download directory:' '%(download)s'
[[ -d '%(sampleset)s' ]] || fail 'No sampleset directory:' '%(sampleset)s'
''' % {'link':link,'basedir':basedir,'download':download,'sampleset':sampleset}, file=sh)


#o.get_projects(space='BSSE_STADLER_COVID', code='STADLER_COVID')[0].get_experiments()
samples=(o.get_experiment(code=expname)
	.get_samples(type=samtype,props={'data_transferred','CONTAINER_PROPERTIES'}))

# NOTE iterating over `samples` is problematic, use pandas dataframe instead
#  - in jupyter, it crashes with "TypeError: argument of type 'int' is not iterable"
#  - in plain python3, it takes ages (probably making server request on each constructor).
#for sa in samples:
	#print("%s\t%s\t%s" % (sa.permId, sa.identifier, sa.p.data_transferred))

status=0
batchtsv={} # dictionary to hold output files per batches
batchlanes={} # dictionary to count lanes used in each batch
# iterate through samples, i.e.: through flow cell lanes
for sa in samples.df.sort_values(by=['registrationDate'],ascending=[True]).itertuples(name='Sample'):
	# parse the container properties for run type and read cycles count
	try:
		m=rxrun.search(sa.CONTAINER_PROPERTIES).groupdict()
	except:
		print(f"cannot parse: {sa.CONTAINER_PROPERTIES}\n{list(map(lambda x: '%x' % ord(x), sa.CONTAINER_PROPERTIES[0:4]))}")
		m={}
	# in Perl  $+{'groupThatDidntMatch'}  is simply empty, in Python trying to access a non-matched groups throws an exception instead, that's my contrived way to get around.
	rlen=int(m['len'])-1 if 'len' in m else ''
	rtype=m['type'] if 'type' in m else ''
	rundate=f"20{m['date']}" if 'date' in m else None # NOTE runfolders are yymmdd, not yyyymmdd
	print(sa.permId, sa.identifier, sa.registrationDate, sa.DATA_TRANSFERRED, rtype, rlen, (m['run'] if 'run' in m else ''), sep='\t')

	lane=cell=seq=''
	try:
		# get the sequencing name, and the flow cell name	original/000000000_CTTKK_1 or HTVCCDRXX_2
		m=rxcell.search(sa.identifier)
		cell=m.group('cell')
		seq=rxclean.sub('_',m.group('seq')) # clean-up seqname from unfriendly caracters
		lane=int(m.group('lane'))
	except:
		print("!!! rx matching failed, cannot detect cell and lane")
		continue

	# batch: date + flow cell, e.g.: '20200426-J3JCY'
	regdate=parser.parse(sa.registrationDate).strftime('%Y%m%d')
	if rundate is None: # fall back: registrationDate
		batch=f"{regdate}_{cell}"
	else:  # ideally, the run folder's date (first six digits)
		if regdate != rundate:
			print(f"\t!!! Batch mismatch: {rundate} vs {regdate}")
		batch=f"{rundate}_{cell}"

	# only process data which was transfered, we don't have access permission for the rest yet
	try:
		datasets=o.get_datasets(sample=sa.permId,props={'EXTERNAL_SAMPLE_NAME','FASTQ_SAMPLE_CODE','BARCODE','INDEX2'}).df
	except:
		print("\t(not accessible yet - cannot open)")
	else:
		# we at least expect to see:
		# - BCL2FASTQ_BASECALLSTATS stats
		# - FASTQ_GZ for unindexed
		# - at least several FASTQ_GZ for indexed samples
		# - FASTQC
		# the last two are required, we don't use the first two.
		skip=False
		if not 'EXTERNAL_SAMPLE_NAME' in datasets.columns:
			print("\t(missing samplenames)", end='')
			skip=True
		else:
			numsam = ((datasets['type']=='FASTQ_GZ') & (datasets['EXTERNAL_SAMPLE_NAME']!='')).sum()
			if numsam < 1: # we need at least 1 named sample
				print(f"\t(nsamples: {numsam})", end='')
				skip=True
		if not (datasets['type']=='FASTQC').any(): # we need FASTQC to be present
			#print(datasets['type']=='FASTQC')
			print("\t(missing fastqc)", end='')
			skip=True
		if skip:
			print("\t(not usable yet - skipping)")
			continue

		tsv=None
		fastqc=None

		if batch in batchtsv:
			if lane in batchlanes[batch]:
				print(f"\t!!! duplicate lane {lane} already seen in {batchlanes[batch][lane]}")
			else:
				batchlanes[batch][lane]=sa.permId

			tsv=batchtsv[batch]
		else:
			batchtsv[batch]=tsv=open(os.path.join(basedir,sampleset,f'samples.{batch}.tsv'), 'wt')
			batchlanes[batch]={lane: sa.permId}

		datasets['sname']=datasets.apply(lambda ds: rxclean.sub('_',ds['EXTERNAL_SAMPLE_NAME']), axis=1) # clean-up samplename from un friendly caracter
		seensname={ }
		# samples are sorted alphanumerically and FASTQC comes before all FASTQ_GZ 
		for ds in datasets.sort_values(by=['type','sname','permId'],ascending=[True,True,False]).itertuples(name='DataSets'):
			if not args.summary:
				print('\t', ds.permId, ds.type, ds.status, ds.registrationDate, ds.sname, ds.EXTERNAL_SAMPLE_NAME,ds.FASTQ_SAMPLE_CODE, ds.BARCODE, ds.INDEX2, sep='\t')
			# skip unavailable data files
			if ds.status == 'AVAILABLE':
				# look for FASTQ files (skip undetermined indexes)
				if ds.type == 'FASTQ_GZ' and ds.EXTERNAL_SAMPLE_NAME:
					# check that we actually have a FastQC
					if fastqc is None:
						print("No FASTQC", file=sys.stderr)
						status |= 1
					# upload error: duplicate samples.
					if ds.sname in seensname:
						# TODO sanity checks if it is indeed the same sample
						print(f"\t\tduplicate {ds.sname} already in {seensname[ds.sname]}", file=sys.stderr)
						continue
					seensname[ds.sname] = ds.permId

					print(ds.sname, batch, rlen, sep='\t', file=tsv)
					# 20200603125141062-60694758/original/BSSE_QGF_139661_000000000_CTTKK_1_MM_1/BSSE_QGF_139661_000000000_CTTKK_1_120000_239_D1_AAGTCGTG_AATTATGC_S97_L001_R1_001_MM_1.fastq.gz	
					print(r'''
[[ -d '%(download)s/%(id)s' ]] || fail 'Not a directory:' '%(download)s/%(id)s'
fastq=( %(download)s/%(id)s/original/%(fqcode)s_%(seq)s%(mm)s/%(fqcode)s_%(seq)s_%(sname)s_%(idx1)s_%(idx2)s_S*_L%(lane)03u_R[1-2]_*%(mm)s.fastq.gz )
[[ "${fastq[*]}" =~ \* ]] && fail 'Cannot list fastq files:' '%(id)s : %(sname)s'
(( ${#fastq[@]} != 2 )) && fail 'Number of fastq files not 2' "${#fastq[@]} : ${fastq[@]}"
mkdir --mode=0770 -p "%(sampleset)s/%(sname)s/%(batch)s/"{raw_data,extracted_data}
for file in "${fastq[@]}"; do
	filename="${file##*/}"
	[[ $file =~ _L%(lane)03u_R[[:digit:]](_[[:digit:]]+%(mm)s.fastq.gz)$ ]] && destname="${filename//${BASH_REMATCH[1]}/.fastq.gz}"
	cp -v ${link} "${file}" "%(sampleset)s/%(sname)s/%(batch)s/raw_data/${destname}"||X
	fqcname="${filename//%(mm)s.fastq.gz/_fastqc.html}"
	[[ $destname =~ _L%(lane)03u_(R[[:digit:]]).fastq.gz$ ]]
	cp -v ${link} "%(download)s/%(fastqc)s/original/%(seq)s/${fqcname}" "%(sampleset)s/%(sname)s/%(batch)s/extracted_data/${BASH_REMATCH[1]}_fastqc.html"||X
done
''' % {'download':download,'sampleset':sampleset,'sname':ds.sname,'batch':batch,'id':ds.permId,'fqcode':ds.FASTQ_SAMPLE_CODE,'seq':seq,'mm':suffix,'idx1':ds.BARCODE,'idx2':ds.INDEX2,'fastqc':fastqc,'lane':lane}, file=sh)
				# look for FASTQC holder directory
				elif ds.type == 'FASTQC':
					# upload error: duplicate samples.
					if fastqc is not None:
						print(f"\t\tduplicate FastQC already in {fastqc}", file=sys.stderr)
						continue
					# 20200603132156916-60694852/original/000000000_CTTKK_1/BSSE_QGF_139567_000000000_CTTKK_1_120162_283_H2_AATGTTCT_AGTCACCT_S4_L001_R1_001_fastqc.html
					fastqc=ds.permId # keep the reference

print("""
if (( ALLOK )); then
	echo All Ok
	exit 0
else
	echo Some errors
	exit 1
fi;
""", file=sh)

if status == 1:
	sys.exit('Some FastQC files still missing')
